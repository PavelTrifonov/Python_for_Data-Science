{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация в решающих деревьях применяется для предотвращения переобучения и улучшения общей способности модели к обобщению на новых данных. В решающих деревьях регуляризация достигается за счет ограничения роста дерева и контроля над его сложностью. Основные параметры, которые подвергаются штрафам, включают следующие:\n",
    "\n",
    "- **Максимальная глубина дерева (max_depth):** Этот параметр ограничивает глубину дерева, то есть максимальное количество уровней от корневого узла до самого глубокого листа. Чем меньше значение этого параметра, тем меньше риск переобучения, так как дерево не сможет слишком сильно подстраиваться под обучающие данные.\n",
    "\n",
    "- **Минимальное количество выборок для разделения узла (min_samples_split):** Этот параметр задает минимальное количество образцов, необходимых для того, чтобы узел мог быть разделен. Если в узле содержится меньше выборок, чем указано в этом параметре, разделение не произойдет. Увеличение этого параметра приводит к уменьшению числа узлов в дереве, что также способствует снижению риска переобучения.\n",
    "\n",
    "- **Минимальное количество выборок в листовом узле (min_samples_leaf):** Определяет минимальное количество выборок, которые должен содержать каждый листовой узел. Это помогает избежать создания узлов с очень малым числом выборок, которые могут быть очень специфичны для обучающего набора данных.\n",
    "\n",
    "- **Максимальное количество листовых узлов (max_leaf_nodes):** Ограничивает общее количество листовых узлов в дереве. Это может сократить сложность дерева и предотвратить его избыточное ветвление.\n",
    "\n",
    "- **Минимальная уменьшение impurity (min_impurity_decrease):** Этот параметр задает минимальное уменьшение импурити (неоднородности), которое должно произойти при разделении узла. Если уменьшение импурити меньше заданного значения, то узел не будет разделен. Это помогает предотвратить разделение узлов на основе незначительных улучшений.\n",
    "\n",
    "- **Максимальная доля признаков (max_features):** Определяет максимальное количество признаков, которые будут рассматриваться для разделения в каждом узле. Это помогает уменьшить вариативность модели и переобучение.\n",
    "\n",
    "Все эти параметры помогают контролировать структуру и сложность дерева, предотвращая его чрезмерное подстраивание под обучающие данные и способствуя лучшей обобщающей способности на новых данных.\n",
    "\n",
    "**L1 и L2 регуляризация** обычно ассоциируются с линейными моделями (такими как линейная регрессия и логистическая регрессия) и нейронными сетями, где они применяются для ограничения весов модели и предотвращения их чрезмерного увеличения. В контексте решающих деревьев и ансамблей на их основе (например, случайных лесов и градиентного бустинга), L1 и L2 регуляризация редко используется, поскольку основные методы регуляризации для этих моделей связаны с ограничением структуры дерева, как было описано ранее.\n",
    "\n",
    "Однако, в некоторых продвинутых реализациях алгоритмов, таких как градиентный бустинг деревьев (например, XGBoost, LightGBM), используются механизмы, аналогичные L1 и L2 регуляризации для контроля веса листьев дерева. Рассмотрим это подробнее:\n",
    "\n",
    "**L1 Регуляризация (LASSO):** В XGBoost и некоторых других реализациях можно применять L1 регуляризацию для штрафования абсолютной величины весов листьев. Это помогает уменьшить количество листьев с ненулевыми весами, что может привести к более простой и обобщающей модели. В XGBoost этот параметр называется alpha.\n",
    "\n",
    "**L2 Регуляризация (Ridge):** L2 регуляризация штрафует квадратичную величину весов листьев, предотвращая их чрезмерный рост. Это помогает избежать больших весов, которые могут привести к переобучению. В XGBoost этот параметр называется lambda.\n",
    "\n",
    "Использование L1 и L2 регуляризации в градиентном бустинге помогает дополнительно контролировать сложность модели и улучшить её обобщающую способность, особенно в задачах с высоким уровнем шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важность признаков (feature importance)** в ансамблях деревьев, таких как случайные леса и градиентный бустинг, рассчитывается на основе различных критериев, которые оценивают вклад каждого признака в построение модели. Основные методы оценки важности признаков включают:\n",
    "\n",
    "- **Важность на основе уменьшения impurity (Gini importance, Mean Decrease in Impurity, MDI)**:\n",
    "Каждый раз, когда в дереве выполняется разделение по какому-либо признаку, рассчитывается уменьшение impurity (например, уменьшение индекса Джини или энтропии).\n",
    "Уменьшение impurity суммируется для каждого признака на всех узлах и всех деревьях ансамбля.\n",
    "Итоговая важность признака пропорциональна суммарному уменьшению impurity, которое было достигнуто благодаря этому признаку.\n",
    "\n",
    "- **Важность на основе перестановок (Permutation importance):**\n",
    "После обучения модели измеряется её производительность на валидационном наборе данных.\n",
    "Затем значения каждого признака поочередно перемешиваются (переставляются случайным образом), и вновь измеряется производительность модели.\n",
    "Важность признака определяется как разница в производительности модели до и после перестановки значений признака. Чем больше ухудшается производительность модели, тем важнее данный признак.\n",
    "\n",
    "- **Важность на основе частоты использования (Mean Decrease in Accuracy, MDA):**\n",
    "Этот метод также основан на изменении точности модели, но здесь измеряется среднее уменьшение точности по всем деревьям, если признак исключается из модели.\n",
    "\n",
    "\n",
    "Рассмотрим пример реализации вычисления важности признаков в случайных лесах с использованием библиотеки scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Важность признаков: [0.08790085 0.02257346 0.41685316 0.47267253]\n",
      "Важность перестановки: [0.01266667 0.01266667 0.166      0.41333333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка данных\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Обучение модели случайного леса\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Получение важности признаков\n",
    "feature_importances = model.feature_importances_\n",
    "print(\"Важность признаков:\", feature_importances)\n",
    "\n",
    "# Перемешивание признаков (Permutation importance)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "permutation_importances = result.importances_mean\n",
    "print(\"Важность перестановки:\", permutation_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, важность признаков в ансамблях деревьев позволяет оценить, какие признаки наиболее сильно влияют на процесс принятия решений моделью и в какой степени они способствуют её точности."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
